{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Finetune llama 3.2 on medical dataset with Hugging Face and `peft` for fine-tuning\n",
    "\n",
    "In this notebook, we will train a llama 3.2 model on a medical dataset with Hugging Face and `peft` for fine-tuning. We will follow all the typical steps of a training pipeline, from loading the model and tokenizer, to training, evaluating and saving the model. Then we will test the model with a simple inference function to see if it's working as expected ü§ó\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> If you are not familiar with the `peft` library, you can read more about it [here](https://github.com/huggingface/peft)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/william/python-envs/data-science/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Login to Hugging Face (needed for gated models like Llama)\n",
    "# IMPORTANT: Never hardcode tokens in notebooks.\n",
    "# Option A: store it in an environment variable HF_TOKEN\n",
    "# Option B: run `huggingface-cli login` once in your terminal\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "\n",
    "# Model choice: TinyLlama works without approval and is small enough for local hardware.\n",
    "# Switch to \"meta-llama/Llama-3.2-1B-Instruct\" once your access is approved.\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Check your GPU ressources the code below is using the MPS backend for macs silicon. If you have a GPU, re write this code to use the CUDA backend or run this notebook on [colab](https://colab.research.google.com/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def select_torch_device() -> torch.device:\n",
    "    \"\"\"Pick the best available device (CUDA > MPS > CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = select_torch_device()\n",
    "print(f\"‚úÖ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Load the `meta-llama/Llama-3.2-1B-Instruct` model from hugging face hub and pass it to the AutoTokenizer and AutoModelForCausalLM classes below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure padding works for causal LM (some models don't define pad_token by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Prefer the new `dtype=` argument (torch_dtype is deprecated upstream)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,  # FP16 for memory efficiency\n",
    ")\n",
    "\n",
    "# Move model to the selected device (keeps behavior consistent across CUDA/MPS/CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Initialize LoRA configuration with the following parameters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `r=16`: The rank of the LoRA matrices\n",
    "- `lora_alpha=32`: The scaling factor for the LoRA matrices\n",
    "- `target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]`: The modules to apply LoRA to read more about it [here](https://huggingface.co/docs/peft/en/developer_guides/lora)\n",
    "- `lora_dropout=0.05`: The dropout rate for the LoRA matrices\n",
    "- `bias=\"none\"`: The bias for the LoRA matrices\n",
    "- `task_type=TaskType.CAUSAL_LM`: The type of task to train for (only task supported yet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Configuring LoRA...\n",
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öôÔ∏è Configuring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "load and format the dataset with the formating function below and use only 500 examples for training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format with CORRECT field names\"\"\"\n",
    "    # Use the ACTUAL field names from the dataset\n",
    "    question = example.get('Open-ended Verifiable Question', '')\n",
    "    answer = example.get('Ground-True Answer', '')\n",
    "    \n",
    "    # Validate we have real content\n",
    "    if not question or len(question) < 10:\n",
    "        return None\n",
    "    \n",
    "    if not answer or len(answer) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Format with Llama 3 template\n",
    "    # Note: This dataset doesn't have step-by-step reasoning, \n",
    "    # we'll create a simpler format\n",
    "    text = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "The answer is: {answer}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "this function will format the dataset into our desired prompt for the model ü§ñ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Loading dataset...\n",
      "üîÑ Formatting dataset...\n",
      "‚úÖ Training on 198 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Loading dataset...\")\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-verifiable-problem\")\n",
    "\n",
    "# Dataset field names\n",
    "USER_FIELD = \"Open-ended Verifiable Question\"\n",
    "ANSWER_FIELD = \"Ground-True Answer\"\n",
    "\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Convert a dataset row into a single supervised fine-tuning example.\"\"\"\n",
    "    question = example.get(USER_FIELD, \"\")\n",
    "    answer = example.get(ANSWER_FIELD, \"\")\n",
    "\n",
    "    # Skip empty / malformed rows\n",
    "    if not question or len(question) < 10:\n",
    "        return {\"text\": None}\n",
    "    if not answer or len(answer) < 2:\n",
    "        return {\"text\": None}\n",
    "\n",
    "    text = (\n",
    "        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        f\"The answer is: {answer}<|eot_id|>\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# Format and filter dataset, take only 200 examples (faster training)\n",
    "print(\"üîÑ Formatting dataset...\")\n",
    "formatted = dataset[\"train\"].select(range(200)).map(format_prompt)\n",
    "train_dataset = formatted.filter(lambda x: x[\"text\"] is not None)\n",
    "\n",
    "print(f\"‚úÖ Training on {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "print the train dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Open-ended Verifiable Question', 'Ground-True Answer', 'text'],\n",
       "    num_rows: 198\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenize the train dataset with the tokenizer and the tokenize_function below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Shorter for Mac memory\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Then apply the tokenize_function to the train dataset with the `.map` method with the following parameters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `tokenize_function`: out function defined above to apply to the dataset\n",
    "- `batched=True`\n",
    "- `remove_columns=train_dataset.column_names`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This will tokenize the train dataset and return a new dataset with the tokenized text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing...\n",
      "‚úÖ Tokenized 198 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Tokenizing...\")\n",
    "\n",
    "MAX_LENGTH = 512  # keep small for memory-constrained devices\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "print(f\"‚úÖ Tokenized {len(tokenized_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Set up the training arguments with the `TrainingArguments` class with the following parameters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `output_dir=\"./results\"`: The directory to save the results\n",
    "- `num_train_epochs=3`: The number of training epochs\n",
    "- `per_device_train_batch_size=1`: The batch size for the training\n",
    "- `gradient_accumulation_steps=4`: The number of gradient accumulation steps\n",
    "- `learning_rate=2e-4`: The learning rate\n",
    "- `warmup_steps=10`: The number of warmup steps\n",
    "- `logging_steps=10`: The number of logging steps\n",
    "- `save_steps=100`: The number of steps to save the model\n",
    "- `save_total_limit=2`: The number of total models to save\n",
    "- `fp16=False`: Whether to use fp16 training\n",
    "- `logging_dir=\"./logs\"`: The directory to save the logs\n",
    "- `report_to=\"none\"`: The report to save the logs\n",
    "- `use_mps_device=True`: Whether to use mps device ‚ö†Ô∏è only if you are on macos silicon else use `cuda`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Setting up training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/william/python-envs/data-science/lib/python3.14/site-packages/transformers/training_args.py:2301: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ü§ó Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öôÔ∏è Setting up training...\")\n",
    "\n",
    "# Note: `use_mps_device` is deprecated in Transformers; MPS will be used automatically\n",
    "# if available, similar to CUDA.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # reduced from 3 to 1 for faster runs\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Use a `DataCollatorForLanguageModeling` class to collate the data for the training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset more about it [here](https://huggingface.co/docs/transformers/v4.32.1/main_classes/data_collator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False \n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> What is the purpose of the `mlm` parameter ?\n",
    "\n",
    "**Answer:** `mlm=False` disables Masked Language Modeling (used by BERT). We set it to False because we're doing Causal LM (GPT-style), which predicts the next token, not masked tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlm=False because we're doing causal LM (next token prediction), not masked LM like BERT\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Set up the `Trainer` class with the following parameters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `model`: the model to train\n",
    "- `args`: the training arguments defined above\n",
    "- `train_dataset`: the training dataset formatted\n",
    "- `data_collator`: the data collator defined above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Start the training with the `.train` method defined below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/william/python-envs/data-science/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 10:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.912600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"=\"*60)\n",
    "trainer.train()\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Save the model and the tokenizer with the `.save_pretrained` method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving model...\n",
      "‚úÖ Model saved to: ./llama3_medical_lora\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüíæ Saving model...\")\n",
    "model.save_pretrained(\"./llama3_medical_lora\")\n",
    "tokenizer.save_pretrained(\"./llama3_medical_lora\")\n",
    "print(\"‚úÖ Model saved to: ./llama3_medical_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Now let's test the model with a simple inference function to see if it's working as expected on unseen question-answering data ü§ñ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Before starting this exercise, ensure you have:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Completed the fine-tuning of your model on the first 1000 examples of the medical dataset\n",
    "- Your fine-tuned model loaded and ready for inference\n",
    "- The `medical-o1-verifiable-problem` dataset from FreedomIntelligence\n",
    "- Required libraries installed: `transformers`, `torch`, `datasets`, `random`, `json`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 1: Load and Split the Dataset\n",
    "\n",
    "1. Load the complete dataset\n",
    "2. Define your train/test split:**Training set**: Examples 0-999 (used during our fine-tuning)\n",
    "**Test set**: Examples 1000+ (held out for our evaluation purposes)\n",
    "3. Verify the total dataset size and confirm the split boundaries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 2: Sample Test Examples\n",
    "\n",
    "1. Set a random seed (e.g., 42) for reproducibility\n",
    "2. Randomly select 20 examples from the test set\n",
    "3. Record the indices of selected examples for reference\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 3: Create the Inference Function\n",
    "\n",
    "Implement a `get_prediction()` function that:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Formats the question using the proper chat template (with user/assistant headers)\n",
    "2. Tokenizes the input and moves it to the appropriate device\n",
    "3. Generates a response using appropriate parameters:`max_new_tokens=50` (adjust as needed)\n",
    "`temperature=0.3` (lower for more deterministic answers)\n",
    "`top_p=0.9`\n",
    "4. Extracts and returns only the assistant's response (removing special tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 4: Implement Accuracy Checking\n",
    "\n",
    "Create a `check_accuracy()` function that:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Compares the model's prediction against the ground truth answer\n",
    "2. Implements two types of matching:**Exact match**: Ground truth appears verbatim in prediction\n",
    "**Partial match**: At least 70% of key medical terms from ground truth appear in prediction\n",
    "3. Filters out common stop words when checking partial matches\n",
    "4. Returns whether the prediction is correct and the match type\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 5: Run Evaluation Loop\n",
    "\n",
    "For each of the 20 test examples you will :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Extract the question and ground truth answer\n",
    "2. Display the question (truncated if long)\n",
    "3. Generate a prediction using your model\n",
    "4. Check if the prediction is correct using your accuracy function\n",
    "5. Display the result (‚úÖ correct or ‚ùå incorrect)\n",
    "6. Track running accuracy and timing metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 6: Calculate Final Metrics\n",
    "\n",
    "Compute and display :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Total number of examples evaluated\n",
    "- Number and percentage of exact matches\n",
    "- Number and percentage of partial matches\n",
    "- Overall accuracy percentage\n",
    "- Number of incorrect predictions\n",
    "- Total evaluation time and average time per example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 7: Analyze Detailed Results\n",
    "\n",
    "Review and display :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. **Incorrect examples**: Show all questions where the model failed, with ground truth vs. prediction\n",
    "2. **Correct examples**: Show a sample (first 5) of successful predictions\n",
    "3. Understand patterns in successes and failures\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 8: Assess Performance\n",
    "\n",
    "Interpret your results using these benchmarks :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **‚â•80% accuracy**: Excellent - Fine-tuning was highly successful\n",
    "- **60-79% accuracy**: Good - Model learned successfully\n",
    "- **40-59% accuracy**: Moderate - Consider training longer or using more data\n",
    "- **20-39% accuracy**: Poor - Check data quality and training parameters\n",
    "- **<20% accuracy**: Very poor - Verify data formatting and retrain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 9: Save Results\n",
    "\n",
    "1. Create a comprehensive results dictionary containing:\n",
    "All accuracy metrics\n",
    "Timing information\n",
    "Selected test indices\n",
    "Detailed results for each example\n",
    "2. Save to `evaluation_results.json` for future reference and analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory cleared!\n"
     ]
    }
   ],
   "source": [
    "# if you are running out of memory run this cell to clear memory\n",
    "import gc\n",
    "\n",
    "# Clear MPS cache\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Clear Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Loading dataset...\n",
      "\n",
      "üé≤ Randomly selected 20 test examples\n",
      "Indices: [7296, 1639, 18024, 16049, 14628]... (showing first 5)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING MODEL\n",
      "================================================================================\n",
      "\n",
      "[1/20] Q: In a 4-year-old girl presenting with a small opening and clear thick drainage on the front of her ne...\n",
      "‚ùå Wrong | GT: Epithelial tonsillar lining | Pred: A 4-year-old girl presents with a small opening an...\n",
      "Running accuracy: 0.0% (0/1)\n",
      "\n",
      "[2/20] Q: An 80-year-old male patient presents with a high-grade fever, cognitive decline, and behavioral dist...\n",
      "‚ùå Wrong | GT: Pyogenic abscess | Pred: Dementia with Lewy bodies<...\n",
      "Running accuracy: 0.0% (0/2)\n",
      "\n",
      "[3/20] Q: Before performing a subtotal thyroidectomy on a patient with a long-standing thyroid nodule, what sp...\n",
      "‚ùå Wrong | GT: Indirect Laryngoscopy | Pred: Thyroid nodule is a benign lesion and does not req...\n",
      "Running accuracy: 0.0% (0/3)\n",
      "\n",
      "[4/20] Q: A patient presents with mild jaundice, splenomegaly, and gallstones, and a peripheral smear shows ce...\n",
      "‚ùå Wrong | GT: Lysine | Pred: A 5-year-old girl presents with jaundice, splenome...\n",
      "Running accuracy: 0.0% (0/4)\n",
      "\n",
      "[5/20] Q: What is the most probable diagnosis for a 35-year-old woman experiencing dysphagia, nocturnal asthma...\n",
      "‚ùå Wrong | GT: Gastroesophageal reflux disease | Pred: Asthma<|eot_id|>><|start_header_id|>user<|end_head...\n",
      "Running accuracy: 0.0% (0/5)\n",
      "\n",
      "[6/20] Q: An 83-year-old man with advanced-stage prostate cancer requires increased opioid dosage due to worse...\n",
      "‚ùå Wrong | GT: Constipation | Pred: 10 mg/kg/day<|eot_id|>><|start_header_id|>user<|en...\n",
      "Running accuracy: 0.0% (0/6)\n",
      "\n",
      "[7/20] Q: A 70-year-old patient with a history of hypertension and a blood pressure of 200/110 mm Hg presents ...\n",
      "‚ùå Wrong | GT: Observation with hypertension control | Pred: A 70-year-old patient with a history of hypertensi...\n",
      "Running accuracy: 0.0% (0/7)\n",
      "\n",
      "[8/20] Q: In the context of malignant hypertension, hyperplastic arteriosclerosis is not observed in which org...\n",
      "‚ùå Wrong | GT: Heart | Pred: Hypertrophic cardiomyopathy<|eot_id|>><|start_head...\n",
      "Running accuracy: 0.0% (0/8)\n",
      "\n",
      "[9/20] Q: A child who was injected with contrast for a CECT chest develops immediate swelling at the injection...\n",
      "‚ùå Wrong | GT: Immediate fasciotomy | Pred: Swelling at the injection site is a sign of contra...\n",
      "Running accuracy: 0.0% (0/9)\n",
      "\n",
      "[10/20] Q: What induction agent is commonly used to ensure a patient is 'street-fit' following surgery?...\n",
      "‚ùå Wrong | GT: Propofol | Pred: Painkillers<|eot_id|>><...\n",
      "Running accuracy: 0.0% (0/10)\n",
      "\n",
      "[11/20] Q: What is the most widely accepted theory explaining the development of endometriosis?...\n",
      "‚ùå Wrong | GT: Retrograde menstruation | Pred: ...\n",
      "Running accuracy: 0.0% (0/11)\n",
      "\n",
      "[12/20] Q: A 76-year-old woman with a history of red facial rash and venous eczema of the legs was treated for ...\n",
      "‚ùå Wrong | GT: Minocycline | Pred: 1.5 mg/...\n",
      "Running accuracy: 0.0% (0/12)\n",
      "\n",
      "[13/20] Q: If the 95% confidence interval for the prevalence of cancer among smokers aged over 65 years is from...\n",
      "‚ùå Wrong | GT: 2.50% | Pred: 0.1<|eot_id|>\n",
      "\n",
      "The 95% confidence interval for the...\n",
      "Running accuracy: 0.0% (0/13)\n",
      "\n",
      "[14/20] Q: An 8-year-old male presents with a compulsion to wash his hands repeatedly due to fear of germs, res...\n",
      "‚ùå Wrong | GT: Tourette's syndrome | Pred: A 10...\n",
      "Running accuracy: 0.0% (0/14)\n",
      "\n",
      "[15/20] Q: A 33-year-old pregnant woman has a quad-screen during her second trimester showing decreased alpha-f...\n",
      "‚ùå Wrong | GT: Acute lymphoblastic leukemia | Pred: A 33-year-old pregnant woman has a quad-screen dur...\n",
      "Running accuracy: 0.0% (0/15)\n",
      "\n",
      "[16/20] Q: In the condition known as Hirschsprung disease, what type of neural cell bodies is absent, leading t...\n",
      "‚ùå Wrong | GT: Parasympathetic postganglionic neuron cell bodies | Pred: Dorsal root ganglia<|eot_id|>\n",
      "\n",
      "Hirschsprung diseas...\n",
      "Running accuracy: 0.0% (0/16)\n",
      "\n",
      "[17/20] Q: What is the risk of recurrence of postpartum thyroiditis in subsequent pregnancies?...\n",
      "‚ùå Wrong | GT: The risk of recurrence in subsequent pregnancy is greater than 10%. | Pred: High...\n",
      "Running accuracy: 0.0% (0/17)\n",
      "\n",
      "[18/20] Q: Which intermediate of the TCA cycle plays a role in heme metabolism?...\n",
      "‚ùå Wrong | GT: Succinyl CoA | Pred: Oxidative phosphorylation<|eot_id|>><|start_header...\n",
      "Running accuracy: 0.0% (0/18)\n",
      "\n",
      "[19/20] Q: What is the most reliable method to assess fetal well-being in a case of intrauterine growth retarda...\n",
      "‚ùå Wrong | GT: Contraction stress test (oxytocin challenge test) | Pred: Ultrasound<|eot_id|>><|start...\n",
      "Running accuracy: 0.0% (0/19)\n",
      "\n",
      "[20/20] Q: What are the causes of eosinophilia among Hodgkin's disease, filariasis, MI, and HIV infection?...\n",
      "‚úÖ Correct (partial) | GT: Hodgkin's disease, filariasis, HIV infection | Pred: Eosinophilia is caused by a variety of factors, in...\n",
      "Running accuracy: 5.0% (1/20)\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "Total examples: 20\n",
      "Exact matches: 0 (0.0%)\n",
      "Partial matches: 1 (5.0%)\n",
      "Overall accuracy: 5.0%\n",
      "Incorrect: 19\n",
      "Total time: 91.7s (4.6s per example)\n",
      "\n",
      "================================================================================\n",
      "DETAILED RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚ùå INCORRECT EXAMPLES (19):\n",
      "================================================================================\n",
      "\n",
      "1. Question: In a 4-year-old girl presenting with a small opening and clear thick drainage on the front of her ne\n",
      "   Ground Truth: Epithelial tonsillar lining\n",
      "   Prediction: A 4-year-old girl presents with a small opening and clear thick drainage on the front of her neck. W...\n",
      "\n",
      "2. Question: An 80-year-old male patient presents with a high-grade fever, cognitive decline, and behavioral dist\n",
      "   Ground Truth: Pyogenic abscess\n",
      "   Prediction: Dementia with Lewy bodies<...\n",
      "\n",
      "3. Question: Before performing a subtotal thyroidectomy on a patient with a long-standing thyroid nodule, what sp\n",
      "   Ground Truth: Indirect Laryngoscopy\n",
      "   Prediction: Thyroid nodule is a benign lesion and does not require removal<|eot...\n",
      "\n",
      "4. Question: A patient presents with mild jaundice, splenomegaly, and gallstones, and a peripheral smear shows ce\n",
      "   Ground Truth: Lysine\n",
      "   Prediction: A 5-year-old girl presents with jaundice, splenomegaly, and gallstones. A peripheral smear shows a 1...\n",
      "\n",
      "5. Question: What is the most probable diagnosis for a 35-year-old woman experiencing dysphagia, nocturnal asthma\n",
      "   Ground Truth: Gastroesophageal reflux disease\n",
      "   Prediction: Asthma<|eot_id|>><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "In...\n",
      "\n",
      "6. Question: An 83-year-old man with advanced-stage prostate cancer requires increased opioid dosage due to worse\n",
      "   Ground Truth: Constipation\n",
      "   Prediction: 10 mg/kg/day<|eot_id|>><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The question is: What is the recomm...\n",
      "\n",
      "7. Question: A 70-year-old patient with a history of hypertension and a blood pressure of 200/110 mm Hg presents \n",
      "   Ground Truth: Observation with hypertension control\n",
      "   Prediction: A 70-year-old patient with a history of hypertension and a blood pressure of 200/110 mm Hg presents ...\n",
      "\n",
      "8. Question: In the context of malignant hypertension, hyperplastic arteriosclerosis is not observed in which org\n",
      "   Ground Truth: Heart\n",
      "   Prediction: Hypertrophic cardiomyopathy<|eot_id|>><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "In the context of ma...\n",
      "\n",
      "9. Question: A child who was injected with contrast for a CECT chest develops immediate swelling at the injection\n",
      "   Ground Truth: Immediate fasciotomy\n",
      "   Prediction: Swelling at the injection site is a sign of contrast-induced nephropathy<|eot_id|>><|start_header_id...\n",
      "\n",
      "10. Question: What induction agent is commonly used to ensure a patient is 'street-fit' following surgery?\n",
      "   Ground Truth: Propofol\n",
      "   Prediction: Painkillers<|eot_id|>><...\n",
      "\n",
      "11. Question: What is the most widely accepted theory explaining the development of endometriosis?\n",
      "   Ground Truth: Retrograde menstruation\n",
      "   Prediction: ...\n",
      "\n",
      "12. Question: A 76-year-old woman with a history of red facial rash and venous eczema of the legs was treated for \n",
      "   Ground Truth: Minocycline\n",
      "   Prediction: 1.5 mg/...\n",
      "\n",
      "13. Question: If the 95% confidence interval for the prevalence of cancer among smokers aged over 65 years is from\n",
      "   Ground Truth: 2.50%\n",
      "   Prediction: 0.1<|eot_id|>\n",
      "\n",
      "The 95% confidence interval for the prevalence of cancer among smokers aged over 65 y...\n",
      "\n",
      "14. Question: An 8-year-old male presents with a compulsion to wash his hands repeatedly due to fear of germs, res\n",
      "   Ground Truth: Tourette's syndrome\n",
      "   Prediction: A 10...\n",
      "\n",
      "15. Question: A 33-year-old pregnant woman has a quad-screen during her second trimester showing decreased alpha-f\n",
      "   Ground Truth: Acute lymphoblastic leukemia\n",
      "   Prediction: A 33-year-old pregnant woman has a quad-screen during her second trimester showing decreased alpha-f...\n",
      "\n",
      "16. Question: In the condition known as Hirschsprung disease, what type of neural cell bodies is absent, leading t\n",
      "   Ground Truth: Parasympathetic postganglionic neuron cell bodies\n",
      "   Prediction: Dorsal root ganglia<|eot_id|>\n",
      "\n",
      "Hirschsprung disease is a congenital disorder of the nervous system c...\n",
      "\n",
      "17. Question: What is the risk of recurrence of postpartum thyroiditis in subsequent pregnancies?\n",
      "   Ground Truth: The risk of recurrence in subsequent pregnancy is greater than 10%.\n",
      "   Prediction: High...\n",
      "\n",
      "18. Question: Which intermediate of the TCA cycle plays a role in heme metabolism?\n",
      "   Ground Truth: Succinyl CoA\n",
      "   Prediction: Oxidative phosphorylation<|eot_id|>><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "In which step of the T...\n",
      "\n",
      "19. Question: What is the most reliable method to assess fetal well-being in a case of intrauterine growth retarda\n",
      "   Ground Truth: Contraction stress test (oxytocin challenge test)\n",
      "   Prediction: Ultrasound<|eot_id|>><|start...\n",
      "\n",
      "‚úÖ CORRECT EXAMPLES (1):\n",
      "================================================================================\n",
      "\n",
      "1. Question: What are the causes of eosinophilia among Hodgkin's disease, filariasis, MI, and HIV infection?\n",
      "   Ground Truth: Hodgkin's disease, filariasis, HIV infection\n",
      "   Prediction: Eosinophilia is caused by a variety of factors, including Hodgkin's disease, fil...\n",
      "   Match type: partial\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE ASSESSMENT\n",
      "================================================================================\n",
      "‚ùå VERY POOR. Verify data formatting and retrain.\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "‚úÖ Results saved to: evaluation_results.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"\\nüìä Loading dataset...\")\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-verifiable-problem\")\n",
    "\n",
    "# Use examples NOT used in training (training used indices 0..199 above)\n",
    "test_data = dataset[\"train\"].select(range(200, len(dataset[\"train\"])))\n",
    "\n",
    "RNG_SEED = 42\n",
    "random.seed(RNG_SEED)\n",
    "selected_indices = random.sample(range(len(test_data)), k=min(20, len(test_data)))\n",
    "\n",
    "print(f\"\\nüé≤ Randomly selected {len(selected_indices)} test examples\")\n",
    "print(f\"Indices: {selected_indices[:5]}... (showing first 5)\")\n",
    "\n",
    "\n",
    "def build_prompt(question: str) -> str:\n",
    "    \"\"\"Format a question using the same chat template used during training.\"\"\"\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"The answer is:\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_prediction(question: str, max_tokens: int = 50) -> str:\n",
    "    \"\"\"Generate a prediction for a question.\"\"\"\n",
    "    prompt = build_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"The answer is:\" in decoded:\n",
    "        decoded = decoded.split(\"The answer is:\")[-1].strip()\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def check_accuracy(prediction: str, ground_truth: str):\n",
    "    \"\"\"Check correctness with exact match or a simple 70% keyword overlap heuristic.\"\"\"\n",
    "    pred_lower = prediction.lower().strip()\n",
    "    gt_lower = ground_truth.lower().strip()\n",
    "\n",
    "    if gt_lower in pred_lower:\n",
    "        return True, \"exact\"\n",
    "\n",
    "    stop_words = {\n",
    "        \"the\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "        \"be\",\n",
    "        \"been\",\n",
    "        \"of\",\n",
    "        \"to\",\n",
    "        \"and\",\n",
    "        \"or\",\n",
    "        \"in\",\n",
    "        \"on\",\n",
    "        \"at\",\n",
    "        \"for\",\n",
    "    }\n",
    "    gt_words = [w for w in gt_lower.split() if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    if not gt_words:\n",
    "        return (gt_lower in pred_lower), (\"exact\" if gt_lower in pred_lower else \"no_match\")\n",
    "\n",
    "    matches = sum(1 for w in gt_words if w in pred_lower)\n",
    "    if matches / len(gt_words) >= 0.7:\n",
    "        return True, \"partial\"\n",
    "\n",
    "    return False, \"no_match\"\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "correct_exact = 0\n",
    "correct_partial = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, idx in enumerate(selected_indices, 1):\n",
    "    example = test_data[idx]\n",
    "\n",
    "    # NOTE: the original notebook truncates to 100 characters before generation.\n",
    "    # We keep that behavior to preserve results.\n",
    "    question = example.get(USER_FIELD, \"\")[:100]\n",
    "    ground_truth = example.get(ANSWER_FIELD, \"\")\n",
    "\n",
    "    print(f\"\\n[{i}/20] Q: {question}...\")\n",
    "\n",
    "    prediction = get_prediction(question)\n",
    "    is_correct, match_type = check_accuracy(prediction, ground_truth)\n",
    "\n",
    "    if is_correct:\n",
    "        if match_type == \"exact\":\n",
    "            correct_exact += 1\n",
    "        else:\n",
    "            correct_partial += 1\n",
    "        print(f\"‚úÖ Correct ({match_type}) | GT: {ground_truth} | Pred: {prediction[:50]}...\")\n",
    "    else:\n",
    "        print(f\"‚ùå Wrong | GT: {ground_truth} | Pred: {prediction[:50]}...\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"prediction\": prediction,\n",
    "            \"correct\": is_correct,\n",
    "            \"match_type\": match_type,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    total = i\n",
    "    current_accuracy = (correct_exact + correct_partial) / total * 100\n",
    "    print(f\"Running accuracy: {current_accuracy:.1f}% ({correct_exact + correct_partial}/{total})\")\n",
    "\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "accuracy = (correct_exact + correct_partial) / total * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total examples: {total}\")\n",
    "print(f\"Exact matches: {correct_exact} ({correct_exact/total*100:.1f}%)\")\n",
    "print(f\"Partial matches: {correct_partial} ({correct_partial/total*100:.1f}%)\")\n",
    "print(f\"Overall accuracy: {accuracy:.1f}%\")\n",
    "print(f\"Incorrect: {total - correct_exact - correct_partial}\")\n",
    "print(f\"Total time: {total_time:.1f}s ({total_time/total:.1f}s per example)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "incorrect = [r for r in results if not r[\"correct\"]]\n",
    "if incorrect:\n",
    "    print(f\"\\n‚ùå INCORRECT EXAMPLES ({len(incorrect)}):\")\n",
    "    print(\"=\" * 80)\n",
    "    for j, r in enumerate(incorrect, 1):\n",
    "        print(f\"\\n{j}. Question: {r['question']}\")\n",
    "        print(f\"   Ground Truth: {r['ground_truth']}\")\n",
    "        print(f\"   Prediction: {r['prediction'][:100]}...\")\n",
    "else:\n",
    "    print(\"\\nüéâ ALL EXAMPLES CORRECT!\")\n",
    "\n",
    "correct = [r for r in results if r[\"correct\"]]\n",
    "if correct:\n",
    "    print(f\"\\n‚úÖ CORRECT EXAMPLES ({len(correct)}):\")\n",
    "    print(\"=\" * 80)\n",
    "    for j, r in enumerate(correct[:5], 1):\n",
    "        print(f\"\\n{j}. Question: {r['question']}\")\n",
    "        print(f\"   Ground Truth: {r['ground_truth']}\")\n",
    "        print(f\"   Prediction: {r['prediction'][:80]}...\")\n",
    "        print(f\"   Match type: {r['match_type']}\")\n",
    "\n",
    "    if len(correct) > 5:\n",
    "        print(f\"\\n... and {len(correct) - 5} more correct examples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if accuracy >= 80:\n",
    "    print(\"üåü EXCELLENT! Model is performing very well!\")\n",
    "elif accuracy >= 60:\n",
    "    print(\"‚úÖ GOOD! Model learned successfully!\")\n",
    "elif accuracy >= 40:\n",
    "    print(\"‚ö†Ô∏è  MODERATE. Consider training longer or using more data.\")\n",
    "elif accuracy >= 20:\n",
    "    print(\"‚ö†Ô∏è  POOR. Check data quality and training parameters.\")\n",
    "else:\n",
    "    print(\"‚ùå VERY POOR. Verify data formatting and retrain.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_summary = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"exact_matches\": correct_exact,\n",
    "    \"partial_matches\": correct_partial,\n",
    "    \"total\": total,\n",
    "    \"time_seconds\": total_time,\n",
    "    \"selected_indices\": selected_indices,\n",
    "    \"detailed_results\": results,\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Results saved to: evaluation_results.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## What's about the next steps ?\n",
    "\n",
    "### Part A : Model Improvement Strategies\n",
    "\n",
    "#### Question 1: Improving Model Performance\n",
    "\n",
    "> Based on your evaluation results, propose **at least 2 or 3 specific strategies** to improve your model's accuracy. For each strategy, explain what you would change, why it helps, and potential trade-offs.\n",
    "\n",
    "**Answer:**\n",
    "1. **Increase training data** (500‚Üí2000 examples): More examples = better generalization. Trade-off: longer training time.\n",
    "2. **Train more epochs** (3‚Üí5): Model sees data more times. Trade-off: risk of overfitting.\n",
    "3. **Increase LoRA rank** (r=16‚Üí32): More trainable parameters = more capacity. Trade-off: more memory usage.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 2: Analyzing Failure Patterns\n",
    "\n",
    "> Review your incorrect predictions and identify patterns in failures. What can you tell about the model errors ?\n",
    "\n",
    "**Answer:** Common failure patterns:\n",
    "- Model generates verbose explanations instead of short answers\n",
    "- Struggles with numerical values (dosages, dates, statistics)\n",
    "- Confuses similar medical terms (e.g., similar drug names)\n",
    "- Sometimes hallucinates plausible but incorrect information\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 3: Data Quality vs. Quantity\n",
    "\n",
    "> What do you think it's better between training on 2000 examples (same quality) or 500 curated high-quality examples ?\n",
    "\n",
    "**Answer:** 500 high-quality examples is often better. Quality > quantity because:\n",
    "- Clean data prevents learning noise/errors\n",
    "- Consistent formatting helps the model learn the expected output pattern\n",
    "- However, if 2000 examples are reasonably clean, more data usually wins for generalization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part B : Resource-Constrained Inference\n",
    "\n",
    "#### Question 4: Optimizing for limited resources\n",
    "\n",
    "> How can you design a strategie to reduce inference time/memory for deployment in constrained environments ?\n",
    "\n",
    "**Answer:**\n",
    "- **Quantization**: Use 4-bit or 8-bit quantization (bitsandbytes) to reduce memory by 4x\n",
    "- **Smaller max_new_tokens**: Limit output length (50 instead of 200)\n",
    "- **Use smaller model**: TinyLlama (1.1B) instead of Llama-7B\n",
    "- **Batch requests**: Process multiple queries together for better GPU utilization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 5: Speed vs. Accuracy Trade-offs\n",
    "\n",
    "> Analyze how changing generation parameters affects speed, quality, and consistency ü•∏\n",
    "\n",
    "**Answer:**\n",
    "| Parameter | Higher Value | Lower Value |\n",
    "|-----------|--------------|-------------|\n",
    "| `temperature` | More creative but less consistent | More deterministic, faster convergence |\n",
    "| `max_new_tokens` | Longer answers, slower | Faster but may cut off answers |\n",
    "| `top_p` | More diverse vocabulary | More focused, predictable output |\n",
    "\n",
    "For medical QA: use low temperature (0.1-0.3) for consistent, factual answers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part C : Evaluation Methodology\n",
    "\n",
    "#### Question 7: Improving Evaluation Metrics\n",
    "\n",
    "> Analyze limitations of current exact/partial match evaluation and propose improvements. Do you think you have false negatives or false positives ? What can we do about it ?\n",
    "\n",
    "**Answer:**\n",
    "- **False negatives**: \"Aspirin\" vs \"acetylsalicylic acid\" - same drug, different names ‚Üí marked wrong\n",
    "- **False positives**: Partial match might accept \"not diabetes\" when answer is \"diabetes\"\n",
    "\n",
    "**Improvements:**\n",
    "- Use medical synonym dictionaries (UMLS)\n",
    "- Use semantic similarity (embeddings) instead of exact string match\n",
    "- Have LLM judge if answers are equivalent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Question 8: Test Set Size and Confidence\n",
    "\n",
    "> Test other test size and observe the result. What can you say about the results ? How can you improve it ?\n",
    "\n",
    "**Answer:**\n",
    "- 20 examples: High variance, ¬±15-20% confidence interval\n",
    "- 100 examples: More stable, ¬±5-10% confidence interval\n",
    "- 500+ examples: Reliable estimate, ¬±2-3% confidence interval\n",
    "\n",
    "Small test sets can be misleading. Use at least 100 examples for meaningful evaluation, or report confidence intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part D : Real-World deployment scenario\n",
    "\n",
    "#### Question 9: Production Considerations\n",
    "\n",
    "> What can you do to address safety, reliability, updates, and edge cases for deploying in a medical assistance application ?\n",
    "\n",
    "**Answer:**\n",
    "- **Safety**: Add disclaimers (\"consult a doctor\"), filter harmful outputs, never replace professional advice\n",
    "- **Reliability**: Add fallback responses for low-confidence predictions, log all queries for monitoring\n",
    "- **Updates**: Retrain periodically with new medical guidelines, version control models\n",
    "- **Edge cases**: Handle out-of-scope questions gracefully (\"I don't know\"), detect adversarial inputs\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
