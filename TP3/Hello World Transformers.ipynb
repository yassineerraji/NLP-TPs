{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hello world Transformers ðŸ¤—\n",
    "\n",
    "In this notebook we will explore the basics of the Hugging Face library by using a pre-trained model to classify text.\n",
    "\n",
    "\n",
    "> âš ï¸ Do not forget to install the `transformers` library to run this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Quick overview of Transformer applications\n",
    "\n",
    "Let's start by defining a text that we will use to test the model.\n",
    "\n",
    "\n",
    "> For testing purposes, we will use a text that is a complaint about a \n",
    "> product. You can generate your own text or change the text to test the \n",
    "> model with different inputs ðŸ¤“\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text we will reuse across tasks (classification, NER, QA, etc.)\n",
    "text = (\n",
    "    \"Dear Amazon, last week I ordered an Optimus Prime action figure \"\n",
    "    \"from your online store in Germany. Unfortunately, when I opened the package, \"\n",
    "    \"I discovered to my horror that I had been sent an action figure of Megatron \"\n",
    "    \"instead! As a lifelong enemy of the Decepticons, I hope you can understand my \"\n",
    "    \"dilemma. To resolve the issue, I demand an exchange of Megatron for the \"\n",
    "    \"Optimus Prime figure I ordered. Enclosed are copies of my records concerning \"\n",
    "    \"this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text Classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“š Question 1: Understanding Pipelines\n",
    "\n",
    "Before we start using the models, let's understand what we're working with:\n",
    "\n",
    "\n",
    "1. What is a `pipeline` in Hugging Face Transformers? What does it abstract away from the user?\n",
    "2. Visit the [pipeline documentation](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial) and list at least 3 other tasks (besides text-classification) that are available.\n",
    "3. What happens when you don't specify a model in the pipeline? How can you specify a specific model?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "1. A pipeline is a tool that handles preprocessing, the model, and postprocessing for you. It makes things easy.\n",
    "2. Summarization, Translation, Question Answering.\n",
    "3. It loads a default model. You can pick one with `model=\"name\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First thing we will do is to classify the text into two categories: positive or negative.\n",
    "\n",
    "\n",
    "To do this, we will use a pre-trained model from the Hugging Face library.\n",
    "\n",
    "\n",
    "We will use the `pipeline` function to load the model and the `text-classification` task.\n",
    "\n",
    "\n",
    "See the documentation for more details: [https://huggingface.co/docs/transformers/main/en/pipeline_tutorial](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sentiment analysis (text classification). Not specifying a model will load a default one.\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“š Question 2: Text Classification Deep Dive\n",
    "\n",
    "Now that you've seen text classification in action, explore further:\n",
    "\n",
    "\n",
    "1. What is the default model used for text-classification? Look at the output above to find its name, then search for it on the [Hugging Face Model Hub](https://huggingface.co/models).\n",
    "2. What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
    "3. The output includes a `score` field. What does this score represent? What range of values can it have?\n",
    "4. **Challenge**: Find a different text-classification model on the Hub that classifies emotions (not just positive/negative). What is its name?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "1. `distilbert-base-uncased-finetuned-sst-2-english`.\n",
    "2. SST-2 dataset (movie reviews). Works best with English sentences.\n",
    "3. It's the confidence score (probability), from 0 to 1.\n",
    "4. `j-hartmann/emotion-english-distilroberta-base`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pipelines return a list of dicts; a DataFrame makes it easier to read.\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Named Entity Recognition\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879010</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556568</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590258</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498348</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775361</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879010         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556568           Mega    208  212\n",
       "4          PER  0.590258         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498348        ##icons    259  264\n",
       "7         MISC  0.775361       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named Entity Recognition (token classification)\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“š Question 3: Named Entity Recognition (NER)\n",
    "\n",
    "Let's understand NER better:\n",
    "\n",
    "\n",
    "1. What does the `aggregation_strategy=\"simple\"` parameter do in the NER pipeline? Check the [token classification documentation](https://huggingface.co/docs/transformers/main/en/tasks/token_classification).\n",
    "2. Looking at the output above, what do the entity types mean? (ORG, MISC, LOC, PER)\n",
    "3. Why do some words appear with `##` prefix (like `##tron` and `##icons`)? What does this indicate about tokenization?\n",
    "4. The model seems to have split \"Megatron\" and \"Decepticons\" \n",
    "incorrectly. Why might this happen? What does this tell you about the \n",
    "model's training data?\n",
    "5. **Challenge**: Find the model card for `dbmdz/bert-large-cased-finetuned-conll03-english`. What is the CoNLL-2003 dataset?\n",
    "\n",
    "\n",
    "> ðŸ¤” How might the choice of tokenizer affect NER performance?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "1. It glues sub-words back together into whole words.\n",
    "2. ORG: Organizations, MISC: Miscellaneous, LOC: Locations, PER: People.\n",
    "3. `##` means it's a part of a word. The tokenizer splits rare words.\n",
    "4. The model doesn't know these specific Transformers names (Megatron, Decepticons).\n",
    "5. CoNLL-2003 is a news dataset used to train NER models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question Answering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extractive question answering (answer span is extracted from the context)\n",
    "reader = pipeline(\"question-answering\")\n",
    "\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“š Question 4: Question Answering Systems\n",
    "\n",
    "Explore how question answering works:\n",
    "\n",
    "\n",
    "1. What type of question answering is this? (Extractive vs. Generative) Check the [question answering documentation](https://huggingface.co/docs/transformers/main/en/tasks/question_answering).\n",
    "2. The model outputs `start` and `end` indices. What do these represent? Why are they important?\n",
    "3. What is the SQuAD dataset? (Look up the model `distilbert-base-cased-distilled-squad` on the Hub)\n",
    "4. Try to think of a question this model CANNOT answer based on the text. Why would it fail?\n",
    "5. **Challenge**: What's the difference between extractive\n",
    " and generative question answering? Find an example of a generative QA \n",
    "model on the Hub.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "1. Extractive (it finds the answer inside the text).\n",
    "2. The start and end positions of the answer in the text.\n",
    "3. SQuAD is a dataset of Wikipedia articles and questions.\n",
    "4. \"What color is Optimus?\" (Not in the text). It would fail.\n",
    "5. Extractive copies text. Generative writes new text. Example: `google/flan-t5-base`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summarization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“š Question 5: Text Summarization\n",
    "\n",
    "Before running the summarization code, let's understand how it works:\n",
    "\n",
    "\n",
    "1. What is the difference between **extractive** and **abstractive** summarization? Check the [summarization documentation](https://huggingface.co/docs/transformers/main/en/tasks/summarization).\n",
    "2. Looking at the code in the next cell, what is the default model used for summarization? Search for it on the [Hugging Face Model Hub](https://huggingface.co/models) and determine:\n",
    "\n",
    "\n",
    "- Is it an extractive or abstractive model?\n",
    "- What architecture does it use? (Hint: look at the model name)\n",
    "- What dataset was it trained on?\n",
    "3. What do the `max_length` and `min_length` parameters control? What happens if `min_length` > `max_length`?\n",
    "4. The parameter `clean_up_tokenization_spaces=True` is used. What does this parameter do? Why might it be useful for summarization?\n",
    "5. **Challenge**: Find two different summarization models on the Hub:\n",
    "\n",
    "\n",
    "- One optimized for short texts (like news articles)\n",
    "- One that can handle longer documents\n",
    "\n",
    "\n",
    "Compare their architectures and training data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "1. Extractive copies sentences. Abstractive writes a new summary.\n",
    "2. `distilbart-cnn-12-6`. Abstractive. Encoder-Decoder. Trained on CNN/Daily Mail.\n",
    "3. Controls the size of the summary.\n",
    "4. Removes bad spacing (like \" . \" becoming \".\").\n",
    "5. Short: `t5-small`. Long: `google/bigbird-pegasus-large-arxiv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Your min_length=56 must be inferior than your max_length=45.\n",
      "/Users/yassine/Desktop/TP3/venv3/lib/python3.13/site-packages/transformers/generation/utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "# Abstractive summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Note: some summarization models have a default min_length; with max_length=45\n",
    "# you may see a warning (it's harmless for this demo).\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Translation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“š Question 6: Machine Translation\n",
    "\n",
    "Let's explore how translation models work:\n",
    "\n",
    "\n",
    "1. What is the architecture behind the `Helsinki-NLP/opus-mt-en-de` model? Look it up on the [Model Hub](https://huggingface.co/Helsinki-NLP/opus-mt-en-de).- What does \"OPUS\" stand for?\n",
    "- What does \"MT\" stand for?\n",
    "2. How would you find a model to translate from English to French? Visit the [translation documentation](https://huggingface.co/docs/transformers/main/en/tasks/translation) and the Model Hub to find at least 2 different models.\n",
    "3. What is the difference between **bilingual** and **multilingual** translation models? What are the advantages and disadvantages of each?\n",
    "4. In the code, we specify the task as `\"translation_en_to_de\"`. How does this relate to the model we're loading?\n",
    "5. The output shows a warning about `sacremoses`. What is this library used for in NLP? Check the [MarianMT documentation](https://huggingface.co/docs/transformers/model_doc/marian).\n",
    "6. **Challenge**: Find a multilingual model (like mBART or\n",
    " M2M100) that can translate between multiple language pairs. How many \n",
    "language pairs does it support?\n",
    "\n",
    "\n",
    "> ðŸŒ What challenges exist for low-resource languages?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "1. MarianMT architecture. OPUS is a huge dataset, MT means Machine Translation.\n",
    "2. `Helsinki-NLP/opus-mt-en-fr`.\n",
    "3. Bilingual = 2 languages (often better). Multilingual = many languages.\n",
    "4. It tells the pipeline what task to do.\n",
    "5. It's a tool to process text for translation.\n",
    "6. `facebook/m2m100_418M`. Supports 100 languages. Harder for languages with less data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Machine translation (English -> German)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m translator = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtranslation_en_to_de\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHelsinki-NLP/opus-mt-en-de\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m outputs = translator(text, clean_up_tokenization_spaces=\u001b[38;5;28;01mTrue\u001b[39;00m, min_length=\u001b[32m100\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtranslation_text\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TP3/venv3/lib/python3.13/site-packages/transformers/pipelines/__init__.py:1078\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1080\u001b[39m         tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TP3/venv3/lib/python3.13/site-packages/transformers/pipelines/__init__.py:1073\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001b[32m   1071\u001b[39m             tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m         tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TP3/venv3/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1180\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1178\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1179\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33min order to use this tokenizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1183\u001b[39m             )\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1186\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1187\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1188\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "# Machine translation (English -> German)\n",
    "translator = pipeline(\n",
    "    \"translation_en_to_de\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-de\",\n",
    ")\n",
    "\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0][\"translation_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text Generation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“š Question 7: Text Generation\n",
    "\n",
    "Understand how language models generate text:\n",
    "\n",
    "\n",
    "1. What is the default model used for text generation in the code below? Look it up on the Hub and answer:- What architecture does GPT-2 use? (decoder-only, encoder-decoder, or encoder-only?)\n",
    "- How many parameters does the base GPT-2 model have?\n",
    "- What type of generation does it perform? (autoregressive, non-autoregressive, etc.)\n",
    "2. Why do we use `set_seed(42)` before generation? What would happen without it? Check the [generation documentation](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation).\n",
    "3. The code uses `max_length=200`. What other parameters can control text generation? Research and explain:- `temperature`\n",
    "- `top_k`\n",
    "- `do_sample`\n",
    "4. Looking at the output, you can see a warning about truncation. What does this mean? Why is the input being truncated?\n",
    "5. What does `pad_token_id` being set to `eos_token_id` mean? Why is this necessary for GPT-2?\n",
    "6. What are the trade-offs between model size and generation quality?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "1. GPT-2. Decoder-only. 124M parameters. Autoregressive (predicts next word).\n",
    "2. To get the exact same result every time.\n",
    "3. Temperature (creativity), Top_k (limits choices), Do_sample (enables randomness).\n",
    "4. The text is too long for the model.\n",
    "5. GPT-2 has no padding token, so we use the EOS token instead.\n",
    "6. Bigger models are smarter but slower and need more RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# Fix randomness so generation is reproducible between runs.\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf64fec61394646bf1be4caeb2f997a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7460663a24b4df2a9a08e41e4d3068d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7e6729fa7e491085b7b10dd1ba0042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06fc0d48c6748b1bc268278fa0443f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbc236e638546318fc62ea695a50650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a98fc91fb9a4b469885e94c1a37a5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66acd07d2704429c88f89940f41c9836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. I did purchase the right size Optimus Prime figure for me, but I did not receive the correct size shipment. I appreciate your patience as I have been working hard to resolve this issue. I have made many attempts at sorting out this issue, but I have not had a single success.\n",
      "\n",
      "Message for my readers with questions and concerns:\n",
      "\n",
      "Dear Customer,\n",
      "\n",
      "In case you need help sorting out the problem of my order, please look here. I will do my best to answer your questions.\n",
      "\n",
      "Please also consider purchasing the Optimus Prime action figure from your online store.\n",
      "\n",
      "As a lifelong ally of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "\n",
      "Dear Customer,\n",
      "\n",
      "In case you need help sorting out the problem of my order, please look here. I will do my best to answer your questions.\n",
      "\n",
      "Please also consider purchasing the Optimus Prime action figure from your online store.\n",
      "\n",
      "As a lifelong ally of the Decepticons, I hope you can\n"
     ]
    }
   ],
   "source": [
    "# Text generation (causal language modeling)\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Change the model inside the pipeline to see other models. Try also other languages ðŸŒ\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
